# Question-Answering-with-a-Fine-Tuned-BERT
BERT is a machine learning framework for natural language processing that is open source (NLP).
BERT is designed to help computers understand the meaning of ambiguous language in text by establishing context through the use of surrounding text to establish context. The BERT framework was trained using Wikipedia text and can be fine-tuned using question and answer datasets. BERT (Bidirectional Encoder Representations from Transformers) is based on Transformers, a deep learning model in which each output element is connected to each input element, and the weightings between them are dynamically calculated based on their connection. You should absolutely fine-tune BERT on your own dataset for anything like text categorization. For question answering, however, it seems like you may be able to get decent results using a model that's already been fine-tuned on the SQuAD benchmark. We'll do just that in this project, and test how well it works with text that isn't in the SQuAD dataset.
Keywords â€” Natural Language Processing, BERT, Question answering, SQuAD dataset.

Question Answering systems are becoming more and more important and popular nowadays as it helps many domains in the industry as they can benefit from it effectively. When people talk about "Question Answering" as a BERT application, they're really talking about using BERT on the Stanford Question Answering Dataset (SQuAD).These systems contain special restrictions to ensure the quality of their material, in addition to standard user guidelines.Users can utilise Question Answering (QA) systems to get specific responses to inquiries given in natural language.The goal of this research is to discover QA methodologies, tools, and systems, as well as the metrics and indicators used to assess these approaches for QA systems, and to figure out how the link between Question Answering and Natural Language Processing is established. The Question answering model is developed by BERT. BERT's main technological breakthrough is the use of Transformer's bidirectional training to language modelling. Transformer is a popular attention model. BERT generally makes use of transformers. Transformer is an attention mechanism that learns contextual relationships between words (or sub-words) in a text.
